{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijPhcVzPXQDU"
      },
      "source": [
        "Demonstrate how `AI Edge Quantizer` can be used to do various quantization experiment with `isnet` (http://arxiv.org/abs/2108.12382)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMejm-XBkfbI"
      },
      "source": [
        "#Install Necessary Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RqZd4zYZdbS"
      },
      "outputs": [],
      "source": [
        "!pip install ai-edge-torch-nightly\n",
        "!pip install ai-edge-quantizer-nightly\n",
        "!pip install pillow requests matplotlib\n",
        "!pip install ai-edge-model-explorer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L16meLkvXgZk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import skimage\n",
        "import tensorflow as tf\n",
        "import ai_edge_quantizer\n",
        "import model_explorer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_i0HmTC_UMe"
      },
      "outputs": [],
      "source": [
        "# @title Preprocess/postprocess utilities (unrelated to quantization) { display-mode: \"form\" }\n",
        "MODEL_INPUT_HW = (1024, 1024)\n",
        "\n",
        "def make_channels_first(image):\n",
        "  image = tf.transpose(image, [2, 0, 1])\n",
        "  image = np.expand_dims(image, axis=0)\n",
        "  return image\n",
        "\n",
        "def preprocess_image(file_path):\n",
        "  image = skimage.io.imread(file_path)\n",
        "  image = tf.image.resize(image, MODEL_INPUT_HW).numpy().astype(np.float32)\n",
        "  image = image / 255.0\n",
        "  return make_channels_first(image)\n",
        "\n",
        "def preprocess_image_ai_edge_torch(test_image_path):\n",
        "  image = Image.open(test_image_path)\n",
        "  test_image = np.array(image.resize(MODEL_INPUT_HW, Image.Resampling.BILINEAR))\n",
        "  test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "  return test_image\n",
        "\n",
        "def run_segmentation(image, tflite_model):\n",
        "  \"\"\"Get segmentation mask of the image.\"\"\"\n",
        "  interpreter = tf.lite.Interpreter(model_path=tflite_model)\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  input_details = interpreter.get_input_details()[0]\n",
        "  interpreter.set_tensor(input_details[\"index\"], image)\n",
        "  interpreter.invoke()\n",
        "\n",
        "  output_details = interpreter.get_output_details()\n",
        "  output_index = 0\n",
        "  outputs = []\n",
        "  for detail in output_details:\n",
        "    outputs.append(interpreter.get_tensor(detail[\"index\"]))\n",
        "  mask = tf.squeeze(outputs[output_index])\n",
        "  # Min-max normalization.\n",
        "  tf_min = np.min(mask)\n",
        "  tf_max = np.max(mask)\n",
        "  mask = (mask - tf_min) / (tf_max - tf_min)\n",
        "  # Scale [0, 1] -\u003e [0, 255].\n",
        "  mask = (mask * 255)\n",
        "  return mask\n",
        "\n",
        "\n",
        "def draw_segementation(image, float_mask, quant_mask, info):\n",
        "  _, ax = plt.subplots(1, 3, figsize=(15, 10))\n",
        "\n",
        "  ax[0].imshow(np.array(image))\n",
        "  ax[1].imshow(np.array(float_mask), cmap=\"gray\")\n",
        "  ax[2].imshow(np.array(quant_mask), cmap=\"gray\")\n",
        "\n",
        "  ax[1].set_title(\"Image\")\n",
        "  ax[1].set_title(\"Float Mask\")\n",
        "  ax[2].set_title(\"Quant Mask: {}\".format(info))\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def save_tfl_model(model_content, save_path):\n",
        "  with gfile.GFile(save_path, \"wb\") as f:\n",
        "    f.write(model_content)\n",
        "  print(\"model saved to: {}\".format(save_path))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7PH4Um5EFkF"
      },
      "outputs": [],
      "source": [
        "!curl -H 'Accept: application/vnd.github.v3.raw'  -O   -L https://api.github.com/repos/google-ai-edge/ai-edge-quantizer/contents/colabs/test_data/input_image.jpg\n",
        "\n",
        "IMAGE_PATH = '/content/input_image.jpg'\n",
        "\n",
        "test_image = preprocess_image_ai_edge_torch(IMAGE_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHsfVmAPu_8H"
      },
      "source": [
        "#Getting TFlite model From Pytorch.\n",
        "\n",
        "ref: https://github.com/google-ai-edge/ai-edge-torch/blob/main/test/image_segmentation/colab/isnet_tfl.ipynb\n",
        "\n",
        "AI Edge quantizer takes a float TFlite and produces a quantize TFlite model, so our first step is build the float TFlite model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KJmCL_h63AMO"
      },
      "outputs": [],
      "source": [
        "# @title Clone IS-Net DIS repo and download Pytorch model\n",
        "\n",
        "%cd /content\n",
        "!rm -rf DIS sample_data\n",
        "\n",
        "!git clone https://github.com/xuebinqin/DIS.git\n",
        "%cd DIS/IS-Net/\n",
        "\n",
        "!curl -o ./model.tar.gz -L https://www.kaggle.com/api/v1/models/paulruiz/dis/pyTorch/8-17-22/1/download\n",
        "!tar -xvf 'model.tar.gz'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hZkC1aB3TiZ"
      },
      "outputs": [],
      "source": [
        "# @title Build torch model\n",
        "\n",
        "import torch\n",
        "from models import ISNetDIS\n",
        "\n",
        "\n",
        "pytorch_model_filename = 'isnet-general-use.pth'\n",
        "pt_model = ISNetDIS()\n",
        "pt_model.load_state_dict(\n",
        "    torch.load(pytorch_model_filename, map_location=torch.device('cpu'))\n",
        ")\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.transforms.functional import normalize\n",
        "\n",
        "\n",
        "class ImageSegmentationModelWrapper(nn.Module):\n",
        "\n",
        "  RESCALING_FACTOR = 255.0\n",
        "  MEAN = 0.5\n",
        "  STD = 1.0\n",
        "\n",
        "  def __init__(self, pt_model):\n",
        "    super().__init__()\n",
        "    self.model = pt_model\n",
        "\n",
        "  def forward(self, image: torch.Tensor):\n",
        "    # BHWC -\u003e BCHW.\n",
        "    image = image.permute(0, 3, 1, 2)\n",
        "\n",
        "    # Rescale [0, 255] -\u003e [0, 1].\n",
        "    image = image / self.RESCALING_FACTOR\n",
        "\n",
        "    # Normalize.\n",
        "    image = (image - self.MEAN) / self.STD\n",
        "\n",
        "    # Get result.\n",
        "    result = self.model(image)[0][0]\n",
        "\n",
        "    # BHWC -\u003e BCHW.\n",
        "    result = result.permute(0, 2, 3, 1)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "wrapped_pt_model = ImageSegmentationModelWrapper(pt_model).eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipvXrW9NWsvy"
      },
      "outputs": [],
      "source": [
        "# @title Convert torch model to TFlite using AI Edge Torch\n",
        "\n",
        "import ai_edge_torch\n",
        "\n",
        "import time\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "FLOAT_MODEL_PATH = os.path.join(BASE_MODEL_PATH, \"isnet_float.tflite\")\n",
        "sample_args = (torch.rand((1, *MODEL_INPUT_HW, 3)),)\n",
        "edge_model = ai_edge_torch.convert(wrapped_pt_model, sample_args)\n",
        "edge_model.export(FLOAT_MODEL_PATH)\n",
        "\n",
        "end = time.time()\n",
        "print(end - start)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aB5IfpVgq_5v"
      },
      "outputs": [],
      "source": [
        "# @title Optional: visualize the model using model explorer\n",
        "model_explorer.visualize(FLOAT_MODEL_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcdAXUn_DeaO"
      },
      "source": [
        "# AI Edge Quantizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S7VLgswL4ig"
      },
      "source": [
        "To use the `Quantizer`, we need to provide\n",
        "* the float .tflite model.\n",
        "* quantization recipe (i.e., apply quantization algorithm X on Operator Y with configuration Z).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcLgjIPExInv"
      },
      "source": [
        "### Quantizing model with dynamic quantization\n",
        "\n",
        "When doing calibration free quantization, Tensorflow lite by default will quantize the weight to int8 format and employ integer execution by quantizing float activation on the fly. This is known as Dynamic Quantization in Tesorflow lite. https://www.tensorflow.org/lite/performance/post_training_quantization#dynamic_range_quantization\n",
        "\n",
        "\n",
        "The following example will showcase how AI Edge Quantizer can achieve the same behaviour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97HBnymXCaCA"
      },
      "outputs": [],
      "source": [
        "from ai_edge_quantizer import recipe\n",
        "\n",
        "# Initialize with float .tflite\n",
        "dyn_qt = ai_edge_quantizer.Quantizer(float_model=FLOAT_MODEL_PATH)\n",
        "\n",
        "dyn_qt.load_quantization_recipe(recipe=recipe.dynamic_wi8_afp32())\n",
        "\n",
        "# we will store the quantized model here\n",
        "DYANMIC_QUANTIZED_MODEL_PATH = os.path.join(BASE_MODEL_PATH, \"isnet_dynamic_wi8_afp32.tflite\")\n",
        "if os.path.exists(DYANMIC_QUANTIZED_MODEL_PATH):\n",
        "  os.remove(DYANMIC_QUANTIZED_MODEL_PATH)\n",
        "\n",
        "# Quantization result contains the quantized model and a copy of the quantization recipe\n",
        "quantization_result = dyn_qt.quantize()\n",
        "quantization_result.save(BASE_MODEL_PATH, \"isnet_dynamic_wi8_afp32\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCc1sgc6uzma"
      },
      "source": [
        "So what's going on? Let take a look step by step:\n",
        "\n",
        "First, we have a prebaked quantization recipe. `dynamic_wi8_afp32` in the name means the recipe will apply dynamic quantization, where we quantize the weight to int8 and activation to float32.\n",
        "\n",
        "Let's take a look at what in this recipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sMvpH5lFYV8"
      },
      "outputs": [],
      "source": [
        "recipe.dynamic_wi8_afp32()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3h8-JbIjNUeU"
      },
      "source": [
        "Here the recipe means: apply the naive min/max uniform algorithm (`min_max_uniform_quantize`) for all ops supported by the AI Edge Quantizer (indicated by `*`) under layers satisfying regex `.*` (i.e., all layers). We want the weights of these ops to be quantized as int8, symmetric, channel_wise, and we want to execute the ops in `Integer` mode without explicitly adding dequantize op.\n",
        "\n",
        "Note: Explicitly adding dequantized op is one way to enable other quantization mechanisms that we will cover later in this colab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvKgDmpQOb-t"
      },
      "source": [
        "`quantization_result` has two components\n",
        "* quantized tflite model (in bytearray) and\n",
        "* the corresponding quantization recipe\n",
        "\n",
        "When we save, we will always save the pair so users know how the model is quantized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDthuSFZOr1W"
      },
      "outputs": [],
      "source": [
        "quantization_result.recipe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUufFeuaN_oN"
      },
      "source": [
        "Now let try running both the float model and the newly quantized model and see how they compare."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_m7OdW0OFXe"
      },
      "outputs": [],
      "source": [
        "quantized_mask = run_segmentation(test_image, DYANMIC_QUANTIZED_MODEL_PATH)\n",
        "float_mask = run_segmentation(test_image, FLOAT_MODEL_PATH)\n",
        "draw_segementation(image, float_mask, quantized_mask, \"AI Edge 8-bit Integer Execution\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgjHZnmTM2uk"
      },
      "source": [
        "### Weight only quantization\n",
        "\n",
        "As shown above, the default solution used by Tensorflow lite didn't give us a good result compared to the original float model. This is because we lose precision when doing integer compute.\n",
        "\n",
        "To increase accuarcy, we can create model that use float compute with quantized constant, otherwise known as weight-only quantization, let's how much we can improve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqUwc6SkSNuF"
      },
      "outputs": [],
      "source": [
        "wo_qt = ai_edge_quantizer.Quantizer(float_model=FLOAT_MODEL_PATH)\n",
        "\n",
        "# Create \u0026 Overwrite quantization recipe\n",
        "tensor_config_8bit = ai_edge_quantizer.qtyping.TensorQuantizationConfig(\n",
        "    num_bits=8,\n",
        "    symmetric=True,\n",
        "    granularity=ai_edge_quantizer.qtyping.QuantGranularity.CHANNELWISE)\n",
        "\n",
        "weight_only_op_config_8bit = ai_edge_quantizer.qtyping.OpQuantizationConfig(\n",
        "    weight_tensor_config=tensor_config_8bit,\n",
        "    compute_precision=ai_edge_quantizer.qtyping.ComputePrecision.FLOAT,\n",
        "    explicit_dequantize=True)\n",
        "\n",
        "wo_qt.update_quantization_recipe(regex=\".*\",\n",
        "                              operation_name=\"*\",\n",
        "                              op_config=weight_only_op_config_8bit,\n",
        "                              algorithm_key=ai_edge_quantizer.algorithm_manager.AlgorithmName.MIN_MAX_UNIFORM_QUANT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFPUztiXlKVE"
      },
      "source": [
        "Here we build a new quantization recipe, for tensor config, we still set them to do int8, symmetric, channel_wise quantization. But this time, we will be adding explcit dequantized into the model, and require the compute to be done in float.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmLekZjkTA4Q"
      },
      "outputs": [],
      "source": [
        "# Quantize.\n",
        "quantization_result = wo_qt.quantize()\n",
        "\n",
        "# Save models\n",
        "WEIGHT_ONLY_MODEL_PATH = os.path.join(BASE_MODEL_PATH, \"isnet_weight_only_wi8_afp32.tflite\")\n",
        "if os.path.exists(WEIGHT_ONLY_MODEL_PATH):\n",
        "  os.remove(WEIGHT_ONLY_MODEL_PATH)\n",
        "quantization_result.save(BASE_MODEL_PATH, \"isnet_weight_only_wi8_afp32\")\n",
        "\n",
        "# Side by side comparison with the float model.\n",
        "quantized_mask = run_segmentation(test_image, WEIGHT_ONLY_MODEL_PATH)\n",
        "draw_segementation(image, float_mask, quantized_mask, \"AI Edge 8-bit Float Execution\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB9YoB86WdY0"
      },
      "source": [
        "With weight only quantization, we have achieved the same quality as the float execution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwexImxzm4CJ"
      },
      "source": [
        "# Debug through Model Explorer (visualization)\n",
        "\n",
        "Now we know that Float execution give us better quality result, but suffer in execution time. Integer execution runs faster but the quality is less adequate.\n",
        "\n",
        "Can we try getting the best of both world?\n",
        "\n",
        "Let's try to figure out where did dynamic execution loses precison first.\n",
        "\n",
        "The following code will generate a tensor-by-tensor comparison result between the dynamic quantized model and original float model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNe85EY7_t6V"
      },
      "outputs": [],
      "source": [
        "# tensor by tensor comparison (float vs. quantized) using median_diff_ratio\n",
        "# as the metric (i.e., mdr = abs(float_tensor - dequantized_tensor)/float_tensor)\n",
        "# and save the results in .json format to visualize through Model Explorer\n",
        "comparion_result = dyn_qt.validate(\n",
        "    signature_test_data=[{'args_0': test_image}], error_metrics='median_diff_ratio', use_reference_kernel=True\n",
        ").save(BASE_MODEL_PATH, \"dynamic\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymibZKyXQUiw"
      },
      "source": [
        "Load `drq_median_diff_ratio.json` on top of the `.tflite` using `Model Explorer` (https://github.com/google-ai-edge/model-explorer) to see how errors propagate through the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lN3dE7qqIBo"
      },
      "outputs": [],
      "source": [
        "config = model_explorer.config()\n",
        "\n",
        "DYNAMIC_NODE_DATA_PATH = os.path.join(BASE_MODEL_PATH, \"dynamic_comparison_result_me_input.json\")\n",
        "\n",
        "(config\n",
        " .add_model_from_path(DYANMIC_QUANTIZED_MODEL_PATH)\n",
        " .add_node_data_from_path(DYNAMIC_NODE_DATA_PATH))\n",
        "\n",
        "model_explorer.visualize_from_config(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnB9x6b4RWJ_"
      },
      "source": [
        "Using Model Explorer, we find that the errors come from the last few layers ('RSU6_stage2d', 'RSU7_stage1d', 'Conv2d_side1'). Lets try not quantize them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u79g4Dxqewmm"
      },
      "source": [
        "## Selective Dynamic Quantization\n",
        "\n",
        "Here we'll override the original `dynamic_wi8_afp32` recipe to skip the three scopes that produce inaccurate results. Notice that for each scope, the newly added rule always take precedence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZID1qD7i2Tn7"
      },
      "outputs": [],
      "source": [
        "scopes = ['RSU6','RSU7','Conv2d_side1']\n",
        "for scope in scopes:\n",
        "  dyn_qt.update_quantization_recipe(\n",
        "      regex=scope,\n",
        "      operation_name=\"CONV_2D\",\n",
        "      algorithm_key='no_quantize',\n",
        "  )\n",
        "dyn_qt.get_quantization_recipe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ng9euL6jLqfJ"
      },
      "outputs": [],
      "source": [
        "SELECTIVE_DYNAMIC_MODEL_PATH = os.path.join(BASE_MODEL_PATH, \"isnet_selective_dynamic_wi8_afp32.tflite\")\n",
        "if os.path.exists(SELECTIVE_DYNAMIC_MODEL_PATH):\n",
        "  os.remove(SELECTIVE_DYNAMIC_MODEL_PATH)\n",
        "\n",
        "dyn_qt.quantize().save(BASE_MODEL_PATH, \"isnet_selective_dynamic_wi8_afp32\")\n",
        "quantized_mask = run_segmentation(test_image, SELECTIVE_DYNAMIC_MODEL_PATH)\n",
        "draw_segementation(image, float_mask, quantized_mask, \"Selective Dynamic\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmJVFPB-RtnT"
      },
      "source": [
        "\n",
        "Can we do better? Lets try to mix `weight-only` with `dynamic`. In this way, we will improve model quality (comparing to full `dynamic quantized` model) while keeping the model size small."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1dwc3PLQuYg"
      },
      "outputs": [],
      "source": [
        "# @title Dynamic Weight-only Mix\n",
        "\n",
        "qt = ai_edge_quantizer.Quantizer(\n",
        "    float_model=FLOAT_MODEL_PATH, quantization_recipe=recipe.dynamic_wi8_afp32()\n",
        ")\n",
        "for scope in scopes:\n",
        "  qt.update_quantization_recipe(\n",
        "      regex=scope,\n",
        "      operation_name=\"CONV_2D\",\n",
        "      op_config=ai_edge_quantizer.qtyping.OpQuantizationConfig(\n",
        "          weight_tensor_config=tensor_config_8bit,\n",
        "          compute_precision=ai_edge_quantizer.qtyping.ComputePrecision.FLOAT,\n",
        "          explicit_dequantize=True,\n",
        "      ),\n",
        "  )\n",
        "# qt.get_quantization_recipe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8J7SM6kOiY6"
      },
      "outputs": [],
      "source": [
        "quantization_result = qt.quantize()\n",
        "\n",
        "MIX_DYNAMIC_WEIGHT_ONLY_MODEL_PATH = os.path.join(BASE_MODEL_PATH, \"isnet_dynamic_weight_only_mix_wi8_afp32.tflite\")\n",
        "if os.path.exists(MIX_DYNAMIC_WEIGHT_ONLY_MODEL_PATH):\n",
        "  os.remove(MIX_DYNAMIC_WEIGHT_ONLY_MODEL_PATH)\n",
        "\n",
        "quantization_result.save(\n",
        "    save_folder=BASE_MODEL_PATH, model_name='isnet_dynamic_weight_only_mix_wi8_afp32'\n",
        ")\n",
        "\n",
        "quantized_mask = run_segmentation(test_image, MIX_DYNAMIC_WEIGHT_ONLY_MODEL_PATH)\n",
        "draw_segementation(image, float_mask, quantized_mask, \"dynamic weight-only mix\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGWu8TFaSk0Y"
      },
      "source": [
        "## Can we do even better?\n",
        "\n",
        "We've seen that int8 weight only essentially gives us similar quality result as the float model. We can try push our boundary and try to use int4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLDp66AIRTlk"
      },
      "outputs": [],
      "source": [
        "# @title INT4 weight-only\n",
        "qt = ai_edge_quantizer.Quantizer(float_model=FLOAT_MODEL_PATH)\n",
        "\n",
        "tensor_config_4bit = ai_edge_quantizer.qtyping.TensorQuantizationConfig(\n",
        "    num_bits=4,\n",
        "    symmetric=False,\n",
        "    granularity=ai_edge_quantizer.qtyping.QuantGranularity.CHANNELWISE)\n",
        "\n",
        "weight_only_op_config_4bit = ai_edge_quantizer.qtyping.OpQuantizationConfig(\n",
        "    weight_tensor_config=tensor_config_8bit,\n",
        "    compute_precision=ai_edge_quantizer.qtyping.ComputePrecision.FLOAT,\n",
        "    explicit_dequantize=True)\n",
        "\n",
        "qt.update_quantization_recipe(\n",
        "    regex=\".*\",\n",
        "    operation_name=\"*\",\n",
        "    op_config=weight_only_op_config_4bit,\n",
        "    algorithm_key=ai_edge_quantizer.algorithm_manager.AlgorithmName.MIN_MAX_UNIFORM_QUANT\n",
        ")\n",
        "quantization_result = qt.quantize()\n",
        "\n",
        "INT_WEIGHT_ONLY_MODEL_PATH = os.path.join(BASE_MODEL_PATH, \"isnet_weight_only_wi4_afp32.tflite\")\n",
        "if os.path.exists(INT_WEIGHT_ONLY_MODEL_PATH):\n",
        "  os.remove(INT_WEIGHT_ONLY_MODEL_PATH)\n",
        "\n",
        "quantization_result.save(\n",
        "    save_folder=BASE_MODEL_PATH, model_name='isnet_weight_only_wi4_afp32'\n",
        ")\n",
        "\n",
        "quantized_mask = run_segmentation(test_image, INT_WEIGHT_ONLY_MODEL_PATH)\n",
        "draw_segementation(image, float_mask, quantized_mask, \"int4 weight only\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//learning/grp/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1Ea3DPWr6VaxT6iHY6zuFqNpMXcHZn2A6",
          "timestamp": 1714413175890
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
